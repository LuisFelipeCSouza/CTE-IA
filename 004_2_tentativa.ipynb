{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f98bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import ast\n",
    "import time\n",
    "\n",
    "# Configuração de dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Semente para reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2209d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('..\\\\te-aprendizado-de-maquina\\\\train.csv')\n",
    "test = pd.read_csv('..\\\\te-aprendizado-de-maquina\\\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função de Distância Real (Haversine) ---\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calcula a distância em KM entre coordenadas.\"\"\"\n",
    "    R = 6371  # Raio da Terra em km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# --- Função de Ruído GPS (Baseado em Erdmann et al.) ---\n",
    "def apply_gps_noise(lats, lons, noise_meters=15.0, prob=1.0):\n",
    "    \"\"\"\n",
    "    Adiciona ruído gaussiano às trajetórias.\n",
    "    noise_meters: desvio padrão do erro em metros.\n",
    "    \"\"\"\n",
    "    # Chance de não aplicar ruído (mantém original)\n",
    "    if np.random.rand() > prob: return lats, lons\n",
    "    \n",
    "    n = len(lats)\n",
    "    # Gera ruído em metros\n",
    "    noise_lat = np.random.normal(0, noise_meters, n)\n",
    "    noise_lon = np.random.normal(0, noise_meters, n)\n",
    "    \n",
    "    # Converte metros para graus\n",
    "    delta_lat = noise_lat / 111111.0\n",
    "    avg_lat = np.radians(np.mean(lats))\n",
    "    delta_lon = noise_lon / (111111.0 * np.cos(avg_lat))\n",
    "    \n",
    "    # Soma e retorna\n",
    "    return (np.array(lats) + delta_lat).tolist(), (np.array(lons) + delta_lon).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4809fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_and_augment_dataset(df, noise_func, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Equilibra o dataset de TREINO gerando cópias ruidosas dos clusters menores.\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando Balanceamento e Data Augmentation ---\")\n",
    "    \n",
    "    # 1. Clusterização rápida baseada no ponto inicial\n",
    "    points = np.array([[row['path_lat'][0], row['path_lon'][0]] for _, row in df.iterrows()])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    \n",
    "    # Trabalhamos numa cópia para não bagunçar o original\n",
    "    df_temp = df.copy()\n",
    "    df_temp['cluster_id'] = kmeans.fit_predict(points)\n",
    "    \n",
    "    # 2. Identificar o tamanho alvo (tamanho do maior cluster)\n",
    "    counts = df_temp['cluster_id'].value_counts()\n",
    "    max_size = counts.max()\n",
    "    print(f\"Contagens Originais:\\n{counts}\")\n",
    "    print(f\"Meta: {max_size} amostras por cluster.\")\n",
    "    \n",
    "    dfs_augmented = []\n",
    "    \n",
    "    # 3. Augmentation por Cluster\n",
    "    for cluster_id in range(n_clusters):\n",
    "        df_c = df_temp[df_temp['cluster_id'] == cluster_id]\n",
    "        \n",
    "        # Adiciona os originais\n",
    "        dfs_augmented.append(df_c) \n",
    "        \n",
    "        # Calcula quantos faltam\n",
    "        n_needed = max_size - len(df_c)\n",
    "        \n",
    "        if n_needed > 0:\n",
    "            print(f\"   -> Cluster {cluster_id}: Gerando {n_needed} cópias sintéticas...\")\n",
    "            # Sorteia amostras existentes para duplicar\n",
    "            samples = resample(df_c, n_samples=n_needed, random_state=42)\n",
    "            \n",
    "            new_rows = []\n",
    "            for _, row in samples.iterrows():\n",
    "                nr = row.copy()\n",
    "                # APLICA O RUÍDO NA CÓPIA\n",
    "                nr['path_lat'], nr['path_lon'] = noise_func(nr['path_lat'], nr['path_lon'])\n",
    "                new_rows.append(nr)\n",
    "            \n",
    "            dfs_augmented.append(pd.DataFrame(new_rows))\n",
    "            \n",
    "    # 4. Finalização\n",
    "    df_final = pd.concat(dfs_augmented).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df_final = df_final.drop(columns=['cluster_id'])\n",
    "    \n",
    "    print(f\"Tamanho Final do Dataset de Treino: {len(df_final)}\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalGridDiscretizer:\n",
    "    def __init__(self, cell_size_meters=200, n_clusters=3):\n",
    "        self.cell_size = cell_size_meters\n",
    "        self.n_clusters = n_clusters\n",
    "        self.grids = {}\n",
    "        self.token_mapper = LabelEncoder()\n",
    "\n",
    "    def fit(self, df):\n",
    "        # Clusterização para definir grades locais\n",
    "        points = np.array([[r['path_lat'][0], r['path_lon'][0]] for _, r in df.iterrows()])\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10).fit(points)\n",
    "        labels = kmeans.labels_\n",
    "        \n",
    "        global_offset = 1\n",
    "        for lbl in set(labels):\n",
    "            c_points = points[labels == lbl]\n",
    "            # Define Bounding Box com margem\n",
    "            lat_min, lon_min = c_points.min(axis=0) - 0.05\n",
    "            lat_max, lon_max = c_points.max(axis=0) + 0.05\n",
    "            \n",
    "            # Calcula passo em graus\n",
    "            lat_step = self.cell_size / 111111.0\n",
    "            lon_step = self.cell_size / (111111.0 * np.cos(np.radians((lat_min+lat_max)/2)))\n",
    "            \n",
    "            n_rows = math.ceil((lat_max - lat_min) / lat_step)\n",
    "            n_cols = math.ceil((lon_max - lon_min) / lon_step)\n",
    "            \n",
    "            self.grids[lbl] = {'lat_min':lat_min, 'lon_min':lon_min, \n",
    "                               'lat_step':lat_step, 'lon_step':lon_step, \n",
    "                               'n_rows':n_rows, 'n_cols':n_cols, 'offset':global_offset}\n",
    "            \n",
    "            global_offset += (n_rows * n_cols)\n",
    "            \n",
    "        # Mapeamento de Tokens (Compactação de IDs)\n",
    "        print(\"   Criando vocabulário de tokens...\")\n",
    "        all_tokens = []\n",
    "        for _, r in df.iterrows():\n",
    "            all_tokens.extend(self._traj_to_raw(r['path_lat'], r['path_lon']))\n",
    "            \n",
    "        self.token_mapper.fit(np.unique(all_tokens))\n",
    "        self.vocab_size = len(self.token_mapper.classes_) + 1\n",
    "        print(f\"   Vocabulário Final: {self.vocab_size} células únicas.\")\n",
    "\n",
    "    def _traj_to_raw(self, lats, lons):\n",
    "        tokens = []\n",
    "        last = None\n",
    "        for lat, lon in zip(lats, lons):\n",
    "            t = 0\n",
    "            # Procura em qual grid o ponto cai\n",
    "            for grid in self.grids.values():\n",
    "                if grid['lat_min'] <= lat < (grid['lat_min'] + grid['n_rows']*grid['lat_step']) and \\\n",
    "                   grid['lon_min'] <= lon < (grid['lon_min'] + grid['n_cols']*grid['lon_step']):\n",
    "                    \n",
    "                    r = int((lat - grid['lat_min']) / grid['lat_step'])\n",
    "                    c = int((lon - grid['lon_min']) / grid['lon_step'])\n",
    "                    t = grid['offset'] + (r * grid['n_cols'] + c)\n",
    "                    break\n",
    "            \n",
    "            # Remove zeros e repetições\n",
    "            if t != 0 and t != last:\n",
    "                tokens.append(t)\n",
    "                last = t\n",
    "        return tokens\n",
    "\n",
    "    def transform(self, lats, lons):\n",
    "        raw = self._traj_to_raw(lats, lons)\n",
    "        valid = [t for t in raw if t in self.token_mapper.classes_]\n",
    "        # Retorna tokens mapeados (+1 pois 0 é PAD)\n",
    "        return (self.token_mapper.transform(valid) + 1).tolist() if valid else [0]\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, df, discretizer, scaler, augment=False, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.discretizer = discretizer\n",
    "        self.scaler = scaler\n",
    "        self.augment = augment\n",
    "        self.mode = mode # 'train', 'val' ou 'test'\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        lats, lons = row['path_lat'], row['path_lon']\n",
    "        \n",
    "        # Augmentation on-the-fly (opcional, se quiser ainda mais variação)\n",
    "        if self.augment: \n",
    "            lats, lons = apply_gps_noise(lats, lons)\n",
    "            \n",
    "        tokens = self.discretizer.transform(lats, lons)\n",
    "        \n",
    "        # Se for teste, retorna target falso (0.0)\n",
    "        if self.mode == 'test':\n",
    "            return torch.tensor(tokens, dtype=torch.long), torch.tensor([0.0, 0.0], dtype=torch.float32)\n",
    "        else:\n",
    "            dest = self.scaler.transform([[row['dest_lat'], row['dest_lon']]]).flatten()\n",
    "            return torch.tensor(tokens, dtype=torch.long), torch.tensor(dest, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_pad = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_stack = torch.stack(targets)\n",
    "    return inputs_pad, targets_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryEncoderAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # 1. Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # 2. LSTM Encoder\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 3. Atenção\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # 4. Regressor (Lat, Lon)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Máscara para ignorar padding\n",
    "        mask = (x != 0).unsqueeze(-1).float()\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        out, _ = self.lstm(embed) # out: [batch, seq, hidden]\n",
    "        \n",
    "        # Cálculo dos pesos de atenção\n",
    "        scores = self.attn(out).masked_fill(mask == 0, -1e9)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        # Vetor de Contexto (Embedding da Trajetória)\n",
    "        context = torch.sum(weights * out, dim=1)\n",
    "        \n",
    "        # Predição de Coordenadas\n",
    "        coords = self.regressor(context)\n",
    "        \n",
    "        return coords, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3004bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correção de Strings para Listas (se necessário)\n",
    "def fix_cols(df):\n",
    "    for c in ['path_lat', 'path_lon']:\n",
    "        if isinstance(df[c].iloc[0], str):\n",
    "            df[c] = df[c].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "# Assumindo que 'train' e 'test' já foram carregados do CSV/Pickle\n",
    "train = fix_cols(train)\n",
    "if 'test' in globals(): test = fix_cols(test)\n",
    "\n",
    "# 2. Split Treino/Validação (ANTES do Augmentation)\n",
    "# Isso garante que a validação seja honesta (apenas dados originais)\n",
    "train_raw, val_raw = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Aplicação do DATA AUGMENTATION (Balanceamento)\n",
    "# Apenas no conjunto de treino raw\n",
    "train_aug = balance_and_augment_dataset(train_raw, apply_gps_noise)\n",
    "\n",
    "# 4. Treinamento do Discretizador e Scaler\n",
    "# Usamos o train_aug para garantir que o vocabulário cubra as variações ruidosas\n",
    "discretizer = GlobalGridDiscretizer(cell_size_meters=200)\n",
    "discretizer.fit(train_aug) \n",
    "\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "target_scaler.fit(train_aug[['dest_lat', 'dest_lon']].values)\n",
    "\n",
    "# 5. Criação dos DataLoaders\n",
    "# ds_train usa augmentation=True para variar ainda mais durante as épocas\n",
    "ds_train = TrajectoryDataset(train_aug, discretizer, target_scaler, augment=True, mode='train')\n",
    "ds_val   = TrajectoryDataset(val_raw, discretizer, target_scaler, augment=False, mode='val')\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dl_val   = DataLoader(ds_val, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Setup concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o Modelo\n",
    "model = TrajectoryEncoderAttention(\n",
    "    vocab_size=discretizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\nIniciando Treinamento...\")\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "epochs = 15 \n",
    "for epoch in range(epochs):\n",
    "    # --- TREINO ---\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dl_train:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred, _ = model(x) # Ignora contexto aqui\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train = total_loss / len(dl_train)\n",
    "    train_losses.append(avg_train)\n",
    "    \n",
    "    # --- VALIDAÇÃO ---\n",
    "    model.eval()\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dl_val:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred, _ = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            total_val += loss.item()\n",
    "            \n",
    "    avg_val = total_val / len(dl_val)\n",
    "    val_losses.append(avg_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train:.5f} | Val Loss: {avg_val:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gráfico da Função de Perda\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Treino (Com Augmentation)', color='blue')\n",
    "plt.plot(val_losses, label='Validação (Dados Originais)', color='orange')\n",
    "plt.title('Curva de Aprendizado')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('MSE Loss (Normalizada)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 2. Cálculo do Erro Real (km) na Validação\n",
    "print(\"\\nCalculando métricas reais de distância...\")\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dl_val:\n",
    "        x = x.to(device)\n",
    "        pred, _ = model(x)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(y.numpy())\n",
    "\n",
    "# Desnormalizar\n",
    "preds_real = target_scaler.inverse_transform(np.vstack(all_preds))\n",
    "targs_real = target_scaler.inverse_transform(np.vstack(all_targets))\n",
    "\n",
    "# Haversine\n",
    "errors_km = haversine_distance(preds_real[:,0], preds_real[:,1], \n",
    "                               targs_real[:,0], targs_real[:,1])\n",
    "\n",
    "mean_error = np.mean(errors_km)\n",
    "median_error = np.median(errors_km)\n",
    "\n",
    "print(f\"Erro Médio (MAE):   {mean_error:.3f} km\")\n",
    "print(f\"Erro Mediano:       {median_error:.3f} km\")\n",
    "\n",
    "# Histograma\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(errors_km, bins=50, color='purple', alpha=0.7)\n",
    "plt.title(\"Distribuição dos Erros em KM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e0845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test' in globals():\n",
    "    print(\"\\nGerando predições para submissão...\")\n",
    "    \n",
    "    # Cria Dataset de Teste (mode='test')\n",
    "    ds_test = TrajectoryDataset(test, discretizer, target_scaler, augment=False, mode='test')\n",
    "    dl_test = DataLoader(ds_test, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    test_preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dl_test:\n",
    "            x = x.to(device)\n",
    "            pred, _ = model(x)\n",
    "            test_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "    # Desnormaliza para Lat/Lon reais\n",
    "    test_preds_real = target_scaler.inverse_transform(np.vstack(test_preds))\n",
    "    \n",
    "    # Cria DataFrame\n",
    "    # Ajuste 'id' conforme a coluna de ID do seu teste\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test.index, \n",
    "        'lat_pred': test_preds_real[:, 0],\n",
    "        'lon_pred': test_preds_real[:, 1]\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(\"submission_kaggle.csv\", index=False)\n",
    "    print(\"Arquivo 'submission_kaggle.csv' salvo com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
